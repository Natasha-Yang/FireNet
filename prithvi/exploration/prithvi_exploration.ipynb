{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prithvi 100M model\n",
    "This notebook will demonstrate basic usage of the Prithvi ViT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with Prithvi - Reconstruction\n",
    "### Get model files\n",
    "\n",
    "To get started, clone the HuggingFace repository for Prithvi 100M, running the command below\n",
    "\n",
    "```bash\n",
    "# Make sure you have git-lfs installed (https://git-lfs.com)\n",
    "git lfs install\n",
    "git clone https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M\n",
    "# rename to a valid python module name\n",
    "mv Prithvi-100M prithvi\n",
    "```\n",
    "\n",
    "Alternatively, you can directly download the [weights](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/tree/main#:~:text=Prithvi_100M.pt,pickle) and [model class](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/Prithvi.py) and [configuration file](https://huggingface.co/ibm-nasa-geospatial/Prithvi-100M/blob/main/Prithvi_100M_config.yaml) from the repository and place them inside a directory named`prithvi`.\n",
    "\n",
    "A third alternative is to leverage the `huggingface_hub` library to download these files directly through code.\n",
    "`%pip install huggingface_hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat it as a module\n",
    "Next, lets add an `__init__.py` file to the downloaded directory, so we can treat it as a module and import the `MaskedAutoencoderViT` class from it.\n",
    "Simply create an empty file inside the `prithvi` directory named `__init__.py` by running the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prithvi/__init__.py\", \"w\") as f:\n",
    "    f.write(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant imports\n",
    "To run this notebook, besides following the installation steps in the [README](./README.md), make sure to install [jupyter](https://jupyter.org/install)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import yaml\n",
    "from prithvi.prithvi_mae import PrithviMAE\n",
    "\n",
    "NO_DATA = -9999\n",
    "NO_DATA_FLOAT = 0.0001\n",
    "PERCENTILES = (0.1, 99.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raster(path, crop=None):\n",
    "    with rasterio.open(path) as src:\n",
    "        img = src.read()\n",
    "\n",
    "        # load first 6 bands\n",
    "        img = img[:6]\n",
    "\n",
    "        img = np.where(img == NO_DATA, NO_DATA_FLOAT, img)\n",
    "        if crop:\n",
    "            img = img[:, -crop[0]:, -crop[1]:]\n",
    "    return img\n",
    "\n",
    "def enhance_raster_for_visualization(raster, ref_img=None):\n",
    "    if ref_img is None:\n",
    "        ref_img = raster\n",
    "    channels = []\n",
    "    for channel in range(raster.shape[0]):\n",
    "        valid_mask = np.ones_like(ref_img[channel], dtype=bool)\n",
    "        valid_mask[ref_img[channel] == NO_DATA_FLOAT] = False\n",
    "        mins, maxs = np.percentile(ref_img[channel][valid_mask], PERCENTILES)\n",
    "        normalized_raster = (raster[channel] - mins) / (maxs - mins)\n",
    "        normalized_raster[~valid_mask] = 0\n",
    "        clipped = np.clip(normalized_raster, 0, 1)\n",
    "        channels.append(clipped)\n",
    "    clipped = np.stack(channels)\n",
    "    channels_last = np.moveaxis(clipped, 0, -1)[..., :3]\n",
    "    rgb = channels_last[..., ::-1]\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_mask_reconstruction(normalized, mask_img, pred_img):\n",
    "    # Mix visible and predicted patches\n",
    "    rec_img = normalized.clone()\n",
    "    rec_img[mask_img == 1] = pred_img[mask_img == 1]  # binary mask: 0 is keep, 1 is remove\n",
    "\n",
    "    mask_img_np = mask_img.numpy().reshape(6, 224, 224).transpose((1, 2, 0))[..., :3]\n",
    "\n",
    "    rec_img_np = (rec_img.numpy().reshape(6, 224, 224) * stds) + means\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "    for subplot in ax:\n",
    "        subplot.axis('off')\n",
    "\n",
    "    ax[0].imshow(enhance_raster_for_visualization(input_data))\n",
    "    masked_img_np = enhance_raster_for_visualization(input_data).copy()\n",
    "    masked_img_np[mask_img_np[..., 0] == 1] = 0\n",
    "    ax[1].imshow(masked_img_np)\n",
    "    ax[2].imshow(enhance_raster_for_visualization(rec_img_np, ref_img=input_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "Assuming you have the relevant files under this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weights\n",
    "weights_path = \"./prithvi/Prithvi_100M.pt\"\n",
    "checkpoint = torch.load(weights_path, map_location=\"cpu\")\n",
    "\n",
    "# read model config\n",
    "model_cfg_path = \"./prithvi/config.yaml\"\n",
    "with open(model_cfg_path) as f:\n",
    "    model_config = yaml.safe_load(f)\n",
    "\n",
    "model_args, train_args = model_config[\"model_args\"], model_config[\"train_params\"]\n",
    "\n",
    "# let us use only 1 frame for now (the model was trained on 3 frames)\n",
    "model_args[\"num_frames\"] = 1\n",
    "\n",
    "# instantiate model\n",
    "model = PrithviMAE(**model_args)\n",
    "model.eval()\n",
    "\n",
    "# load weights into model\n",
    "# strict=false since we are loading with only 1 frame, but the warning is expected\n",
    "del checkpoint['encoder.pos_embed']\n",
    "del checkpoint['decoder.decoder_pos_embed']\n",
    "_ = model.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try it out!\n",
    "We can access the images directly from the HuggingFace space thanks to rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = \"prithvi/examples/HLS.L30.T13REN.2018013T172747.v2.0.B02.B03.B04.B05.B06.B07_cropped.tif\"\n",
    "input_data = load_raster(raster_path, crop=(224, 224))\n",
    "print(f\"Input data shape is {input_data.shape}\")\n",
    "raster_for_visualization = enhance_raster_for_visualization(input_data)\n",
    "plt.imshow(raster_for_visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets call the model!\n",
    "We pass:\n",
    " - The normalized input image, cropped to size (224, 224)\n",
    " - `mask_ratio`: The proportion of pixels that will be masked\n",
    "\n",
    "The model returns a tuple with:\n",
    " - loss\n",
    " - reconstructed image\n",
    " - mask used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics used to normalize images before passing to the model\n",
    "means = np.array(train_args[\"data_mean\"]).reshape(-1, 1, 1)\n",
    "stds = np.array(train_args[\"data_std\"]).reshape(-1, 1, 1)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # normalize image\n",
    "    normalized = image.copy()\n",
    "    normalized = ((image - means) / stds)\n",
    "    normalized = torch.from_numpy(normalized.reshape(1, normalized.shape[0], 1, *normalized.shape[-2:])).to(torch.float32)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = preprocess_image(input_data)\n",
    "with torch.no_grad():\n",
    "        mask_ratio = 0.5\n",
    "        _, pred, mask = model(normalized, mask_ratio=mask_ratio)\n",
    "        mask_img = model.unpatchify(mask.unsqueeze(-1).repeat(1, 1, pred.shape[-1])).detach().cpu()\n",
    "        pred_img = model.unpatchify(pred).detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets use these to build a nice output visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_mask_reconstruction(normalized, mask_img, pred_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with finetuned Prithvi\n",
    "\n",
    "#### Let's explore a finetuned example - Flood Segmentation\n",
    "\n",
    "This time, lets use the huggingface hub library to directly download the files for the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmsegmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmmengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Config\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmmsegmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m build_segmentor\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmmsegmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m Compose, LoadImageFromFile\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmmsegmentation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapis\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m init_segmentor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmsegmentation'"
     ]
    }
   ],
   "source": [
    "from mmengine.config import Config\n",
    "from mmseg.models import build_segmentor\n",
    "from mmseg.datasets.pipelines import Compose, LoadImageFromFile\n",
    "from mmseg.apis import init_segmentor\n",
    "from model_inference import inference_segmentor, process_test_pipeline\n",
    "from huggingface_hub import hf_hub_download\n",
    "import matplotlib\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the config and model weights from huggingface\n",
    "config_path=hf_hub_download(repo_id=\"ibm-nasa-geospatial/Prithvi-100M-sen1floods11\", filename=\"sen1floods11_Prithvi_100M.py\")\n",
    "ckpt=hf_hub_download(repo_id=\"ibm-nasa-geospatial/Prithvi-100M-sen1floods11\", filename='sen1floods11_Prithvi_100M.pth')\n",
    "finetuned_model = init_segmentor(Config.fromfile(config_path), ckpt, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's grab an image to do inference on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/spaces/ibm-nasa-geospatial/Prithvi-100M-sen1floods11-demo/resolve/main/Spain_7370579_S2Hand.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_inference = load_raster(\"Spain_7370579_S2Hand.tif\")\n",
    "print(f\"Image input shape is {input_data_inference.shape}\")\n",
    "raster_for_visualization = enhance_raster_for_visualization(input_data_inference)\n",
    "plt.axis('off')\n",
    "plt.imshow(raster_for_visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt this pipeline for Tif files with > 3 images\n",
    "custom_test_pipeline = process_test_pipeline(finetuned_model.cfg.data.test.pipeline)\n",
    "result = inference_segmentor(finetuned_model, \"Spain_7370579_S2Hand.tif\", custom_test_pipeline=custom_test_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "input_data_inference = load_raster(\"Spain_7370579_S2Hand.tif\")\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax=2)\n",
    "ax[0].imshow(enhance_raster_for_visualization(input_data_inference))\n",
    "ax[1].imshow(result[0], norm=norm, cmap=\"jet\")\n",
    "ax[2].imshow(enhance_raster_for_visualization(input_data_inference))\n",
    "ax[2].imshow(result[0], cmap=\"jet\", alpha=0.3, norm=norm)\n",
    "for subplot in ax:\n",
    "    subplot.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning for your use case\n",
    "To finetune, you can now write a PyTorch loop as usual to train on your dataset. Simply extract the backbone from the model with some surgery and run only the model features forward, with no masking!\n",
    "\n",
    " In general some reccomendations are:\n",
    "- At least in the beggining, experiment with freezing the backbone. This will give you much faster iteration through experiments.\n",
    "- Err on the side of a smaller learning rate\n",
    "- With an unfrozen encoder, regularization is your friend! (Weight decay, dropout, batchnorm...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if going with plain pytorch:\n",
    "# - remember to normalize images beforehand (find the normalization statistics in the config file)\n",
    "# - turn off masking by passing mask_ratio = 0\n",
    "normalized = preprocess_image(input_data)\n",
    "features, _, _ = model.forward_encoder(normalized, mask_ratio=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do these features look like?\n",
    "These are the standard output of a ViT.\n",
    "- Dim 1: Batch size\n",
    "- Dim 2: [`cls_token`] + tokens representing flattened image\n",
    "- Dim 3: embedding dimension\n",
    "\n",
    "First reshape features into \"image-like\" shape:\n",
    "- Drop cls_token\n",
    "- reshape into HxW shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Encoder features have shape {features.shape}\")\n",
    "\n",
    "# drop cls token\n",
    "reshaped_features = features[:, 1:, :]\n",
    "\n",
    "# reshape\n",
    "feature_img_side_length = int(np.sqrt(reshaped_features.shape[1]))\n",
    "reshaped_features = reshaped_features.view(-1, feature_img_side_length, feature_img_side_length, model_args[\"embed_dim\"])\n",
    "# channels first\n",
    "reshaped_features = reshaped_features.permute(0, 3, 1, 2)\n",
    "print(f\"Encoder features have new shape {reshaped_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a segmentation head\n",
    "A simple segmentation head can consist of a few upscaling blocks + a final head for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "upscaling_block = lambda in_channels, out_channels: nn.Sequential(nn.Upsample(scale_factor=2), nn.Conv2d(kernel_size=3, in_channels=in_channels, out_channels=out_channels, padding=1), nn.ReLU())\n",
    "embed_dims = [model_args[\"embed_dim\"] // (2**i) for i in range(5)]\n",
    "segmentation_head = nn.Sequential(\n",
    "    *[\n",
    "    upscaling_block(embed_dims[i], embed_dims[i+1]) for i in range(4)\n",
    "    ],\n",
    "    nn.Conv2d(kernel_size=1, in_channels=embed_dims[-1], out_channels=num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running features through the segmentation head\n",
    "We now get an output of shape [batch_size, num_classes, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_head(reshaped_features).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning - MMSeg\n",
    "Alternatively, finetune using the MMSegmentation extension we have opensourced.\n",
    "- No model surgery required\n",
    "- No need to write boilerplate training code\n",
    "- Integrations with Tensorboard, MLFlow, ...\n",
    "- Segmentation evaluation metrics / losses built in\n",
    "\n",
    "1. Build your config file. Look [here](./configs/) for examples, the [ReadME](./README.md) for some docs and [MMSeg](https://mmsegmentation.readthedocs.io/en/0.x/tutorials/config.html) for more general tutorials.\n",
    "2. Collect your dataset in the format determined by MMSeg\n",
    "3. `mim train mmsegmentation <path to my config>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the model looks like in the MMSeg configuration code.\n",
    "\n",
    "All this composition we did above is done for you!\n",
    "```python\n",
    "model = dict(\n",
    "    type=\"TemporalEncoderDecoder\",\n",
    "    frozen_backbone=False,\n",
    "    backbone=dict(\n",
    "        type=\"TemporalViTEncoder\",\n",
    "        pretrained=pretrained_weights_path,\n",
    "        img_size=img_size,\n",
    "        patch_size=patch_size,\n",
    "        num_frames=num_frames,\n",
    "        tubelet_size=1,\n",
    "        in_chans=len(bands),\n",
    "        embed_dim=embed_dim,\n",
    "        depth=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        norm_pix_loss=False,\n",
    "    ),\n",
    "    neck=dict(\n",
    "        type=\"ConvTransformerTokensToEmbeddingNeck\",\n",
    "        embed_dim=num_frames*embed_dim,\n",
    "        output_embed_dim=embed_dim,\n",
    "        drop_cls_token=True,\n",
    "        Hp=img_size // patch_size,\n",
    "        Wp=img_size // patch_size,\n",
    "    ),\n",
    "    decode_head=dict(\n",
    "        num_classes=num_classes,\n",
    "        in_channels=embed_dim,\n",
    "        type=\"FCNHead\",\n",
    "        in_index=-1,\n",
    "        ignore_index=ignore_index,\n",
    "        channels=256,\n",
    "        num_convs=1,\n",
    "        concat_input=False,\n",
    "        dropout_ratio=0.1,\n",
    "        norm_cfg=norm_cfg,\n",
    "        align_corners=False,\n",
    "        loss_decode=dict(\n",
    "            type=\"CrossEntropyLoss\",\n",
    "            use_sigmoid=False,\n",
    "            loss_weight=1,\n",
    "            class_weight=ce_weights,\n",
    "            avg_non_ignore=True\n",
    "        ),\n",
    "    ),\n",
    "    (...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
