Here's a cleaner and more structured version of your README with improved clarity and formatting:

---

# FireNet  

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>  

**Spatio-temporal Multimodal Wildfire Spread Prediction**  

## Setup  
Install the required dependencies using:  
```bash
pip install -r requirements.txt
```  

## Downloading and Preparing the Dataset  
This repository uses the **WildfireSpreadTS** dataset, which is freely available at [Zenodo](https://zenodo.org/records/8006177).  

To speed up training, convert the dataset to HDF5 format by running:  
```bash
python3 src/preprocess/CreateHDF5Dataset.py --data_dir YOUR_DATA_DIR --target_dir YOUR_TARGET_DIR
```  

## Running Experiments  
Run training with the following command:  
```bash
python3 train.py --config=cfgs/convlstm_cbam/full_run.yaml \
                 --trainer=cfgs/trainer_single_gpu.yaml \
                 --data=cfgs/data_monotemporal_full_features.yaml \
                 --seed_everything=0 \
                 --trainer.max_epochs=200 \
                 --do_test=True \
                 --data.data_dir YOUR_DATA_DIR \
                 --model_name YOUR_MODEL_NAME
```  

## Citations  
- The **WSTS** module was forked from [SebastianGer/WildfireSpreadTS](https://github.com/SebastianGer/WildfireSpreadTS.git).  
- All code related to the **CBAM architecture** is our own.  

---

This version improves readability, formatting, and clarity. Let me know if you need any tweaks! ðŸš€

## Project Organization
The most up-to-date code compatible with the **WildfireSpreadTS** dataset is currently located in **WSTS**. Since we recently transitioned datasets, we are in the process of migrating documents from **WSTS** to our main module, **FireNet**.  

```
â”œâ”€â”€ LICENSE            <- Open-source license if one is chosen
â”œâ”€â”€ Makefile           <- Makefile with convenience commands like `make data` or `make train`
â”œâ”€â”€ README.md          <- The top-level README for developers using this project.
â”œâ”€â”€ WSTS               <- **Latest working code compatible with the WildfireSpreadTS dataset**
â”‚   â”œâ”€â”€ cfgs           <- YAML configuration files for trainer, data loader, and model settings
â”‚   â”œâ”€â”€ src            <- Main working directory containing models and preprocessing code
    â”‚   â”œâ”€â”€ __init__.py         
    â”‚   â””â”€â”€ train.py
    â”‚   â”œâ”€â”€ dataloader         
    â”‚   â”œâ”€â”€ models 
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ external       <- Data from third party sources.
â”‚   â”œâ”€â”€ interim        <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ processed      <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ raw            <- The original, immutable data dump.
â”‚
â”œâ”€â”€ docs               <- A default mkdocs project; see www.mkdocs.org for details
â”‚
â”œâ”€â”€ models             <- Trained and serialized models, model predictions, or model summaries
â”‚
â”œâ”€â”€ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
â”‚                         the creator's initials, and a short `-` delimited description, e.g.
â”‚                         `1.0-jqp-initial-data-exploration`.
â”‚
â”œâ”€â”€ pyproject.toml     <- Project configuration file with package metadata for 
â”‚                         firenet and configuration for tools like black
â”‚
â”œâ”€â”€ references         <- Data dictionaries, manuals, and all other explanatory materials.
â”‚
â”œâ”€â”€ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
â”‚   â””â”€â”€ figures        <- Generated graphics and figures to be used in reporting
â”‚
â”œâ”€â”€ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
â”‚                         generated with `pip freeze > requirements.txt`
â”‚
â”œâ”€â”€ setup.cfg          <- Configuration file for flake8
â”‚
â””â”€â”€ firenet            <- **Outdated source code for the old dataset, to be integrated with WSTS**  
```

--------

